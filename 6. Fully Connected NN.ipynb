{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7c082db",
   "metadata": {},
   "source": [
    "# Mini Project 5 - Classification with Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134d75b5",
   "metadata": {},
   "source": [
    "## 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e253b45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac95711b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n"
     ]
    }
   ],
   "source": [
    "# load the data and print the shape\n",
    "digits = load_digits()\n",
    "print(digits.data.shape)\n",
    "print(digits.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8977c663",
   "metadata": {},
   "source": [
    "## 2. Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c7548ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa91bc91760>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAECCAYAAADXWsr9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAL40lEQVR4nO3dW4hd9RXH8d+vY7xGSaxWJBHtSAmIUHNBKgFpNYpWsS81RFCotCQPrRha0NiX4ptPYh+KELxU8IajBoq01gQVEVrtTIz1MrFoiJhEHSWRGAsR4+rD2SkxnTp7xv3/z5mzvh845MzMmb3WzOR39t7n7L2XI0IABtu3ZrsBAOURdCABgg4kQNCBBAg6kABBBxLoi6DbvsL2W7bftr2hcK37bE/Yfr1knSPqnWX7Odvjtt+wfXPhesfbftn2q02920vWa2oO2X7F9lOlazX1dtp+zfY226OFay2w/bjt7c3f8KKCtZY0P9Ph237b6ztZeETM6k3SkKR3JA1LOlbSq5LOK1jvYknLJL1e6ec7U9Ky5v7Jkv5V+OezpPnN/XmSXpL0g8I/468lPSzpqUq/052STqtU6wFJv2juHytpQaW6Q5I+kHR2F8vrhzX6hZLejogdEfG5pEcl/aRUsYh4QdLeUsufpN77EbG1uf+ppHFJiwrWi4g40Hw4r7kVOyrK9mJJV0m6p1SN2WL7FPVWDPdKUkR8HhGfVCp/qaR3IuLdLhbWD0FfJOm9Iz7epYJBmE22z5G0VL21bMk6Q7a3SZqQtDkiSta7S9Itkr4sWONoIekZ22O21xasMyzpI0n3N7sm99g+qWC9I62R9EhXC+uHoHuSzw3ccbm250t6QtL6iNhfslZEHIqICyQtlnSh7fNL1LF9taSJiBgrsfyvsTIilkm6UtIvbV9cqM4x6u3m3R0RSyV9Jqnoa0iSZPtYSddIGulqmf0Q9F2Szjri48WS9sxSL0XYnqdeyB+KiCdr1W02M5+XdEWhEislXWN7p3q7XJfYfrBQrf+KiD3NvxOSNqm3+1fCLkm7jtgiely94Jd2paStEfFhVwvsh6D/Q9L3bH+3eSZbI+lPs9xTZ2xbvX288Yi4s0K9020vaO6fIGmVpO0lakXEbRGxOCLOUe/v9mxEXF+i1mG2T7J98uH7ki6XVOQdlIj4QNJ7tpc0n7pU0pslah3lOnW42S71Nk1mVUR8YftXkv6q3iuN90XEG6Xq2X5E0g8lnWZ7l6TfRcS9peqpt9a7QdJrzX6zJP02Iv5cqN6Zkh6wPaTeE/ljEVHlba9KzpC0qff8qWMkPRwRTxesd5Okh5qV0A5JNxasJdsnSrpM0rpOl9u8lA9ggPXDpjuAwgg6kABBBxIg6EACBB1IoK+CXvhwxlmrRT3qzXa9vgq6pJq/zKp/OOpRbzbr9VvQARRQ5IAZ2wN9FM7ChQun/T0HDx7UcccdN6N6ixZN/2S+vXv36tRTT51Rvf37p3/OzYEDBzR//vwZ1du9e/e0vyci1BwdN22HDh2a0ffNFRHxP7+YWT8Edi5atWpV1Xp33HFH1XpbtmypWm/DhuInhH3Fvn37qtbrB2y6AwkQdCABgg4kQNCBBAg6kABBBxIg6EACBB1IoFXQa45MAtC9KYPeXGTwD+pdgvY8SdfZPq90YwC602aNXnVkEoDutQl6mpFJwKBqc1JLq5FJzYnytc/ZBdBCm6C3GpkUERslbZQG/zRVYK5ps+k+0COTgAymXKPXHpkEoHutLjzRzAkrNSsMQGEcGQckQNCBBAg6kABBBxIg6EACBB1IgKADCRB0IAEmtcxA7ckpw8PDVevNZOTUN7F3796q9VavXl213sjISNV6k2GNDiRA0IEECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJEDQgQTajGS6z/aE7ddrNASge23W6H+UdEXhPgAUNGXQI+IFSXXPOgDQKfbRgQQ6O02V2WtA/+os6MxeA/oXm+5AAm3eXntE0t8kLbG9y/bPy7cFoEtthixeV6MRAOWw6Q4kQNCBBAg6kABBBxIg6EACBB1IgKADCRB0IIGBmL22fPnyqvVqz0I799xzq9bbsWNH1XqbN2+uWq/2/xdmrwGogqADCRB0IAGCDiRA0IEECDqQAEEHEiDoQAIEHUiAoAMJtLk45Fm2n7M9bvsN2zfXaAxAd9oc6/6FpN9ExFbbJ0sas705It4s3BuAjrSZvfZ+RGxt7n8qaVzSotKNAejOtPbRbZ8jaamkl4p0A6CI1qep2p4v6QlJ6yNi/yRfZ/Ya0KdaBd32PPVC/lBEPDnZY5i9BvSvNq+6W9K9ksYj4s7yLQHoWpt99JWSbpB0ie1tze3HhfsC0KE2s9delOQKvQAohCPjgAQIOpAAQQcSIOhAAgQdSICgAwkQdCABgg4kMBCz1xYuXFi13tjYWNV6tWeh1Vb795kRa3QgAYIOJEDQgQQIOpAAQQcSIOhAAgQdSICgAwkQdCABgg4k0OYqsMfbftn2q83stdtrNAagO22OdT8o6ZKIONBc3/1F23+JiL8X7g1AR9pcBTYkHWg+nNfcGNAAzCGt9tFtD9neJmlC0uaIYPYaMIe0CnpEHIqICyQtlnSh7fOPfozttbZHbY923COAb2har7pHxCeSnpd0xSRf2xgRKyJiRTetAehKm1fdT7e9oLl/gqRVkrYX7gtAh9q86n6mpAdsD6n3xPBYRDxVti0AXWrzqvs/JS2t0AuAQjgyDkiAoAMJEHQgAYIOJEDQgQQIOpAAQQcSIOhAAsxem4EtW7ZUrTfoav/99u3bV7VeP2CNDiRA0IEECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJEDQgQRaB70Z4vCKbS4MCcwx01mj3yxpvFQjAMppO5JpsaSrJN1Tth0AJbRdo98l6RZJX5ZrBUApbSa1XC1pIiLGpngcs9eAPtVmjb5S0jW2d0p6VNIlth88+kHMXgP615RBj4jbImJxRJwjaY2kZyPi+uKdAegM76MDCUzrUlIR8bx6Y5MBzCGs0YEECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJDAQs9dqz9Javnx51Xq11Z6FVvv3OTIyUrVeP2CNDiRA0IEECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJEDQgQRaHQLbXOr5U0mHJH3BJZ2BuWU6x7r/KCI+LtYJgGLYdAcSaBv0kPSM7THba0s2BKB7bTfdV0bEHtvfkbTZ9vaIeOHIBzRPADwJAH2o1Ro9IvY0/05I2iTpwkkew+w1oE+1maZ6ku2TD9+XdLmk10s3BqA7bTbdz5C0yfbhxz8cEU8X7QpAp6YMekTskPT9Cr0AKIS314AECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJOCI6H6hdvcL/RrDw8M1y2l0dLRqvXXr1lWtd+2111atV/vvt2LFYJ+OERE++nOs0YEECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJEDQgQQIOpBAq6DbXmD7cdvbbY/bvqh0YwC603aAw+8lPR0RP7V9rKQTC/YEoGNTBt32KZIulvQzSYqIzyV9XrYtAF1qs+k+LOkjSffbfsX2Pc0gh6+wvdb2qO26p3YBmFKboB8jaZmkuyNiqaTPJG04+kGMZAL6V5ug75K0KyJeaj5+XL3gA5gjpgx6RHwg6T3bS5pPXSrpzaJdAehU21fdb5L0UPOK+w5JN5ZrCUDXWgU9IrZJYt8bmKM4Mg5IgKADCRB0IAGCDiRA0IEECDqQAEEHEiDoQAIDMXuttrVr11atd+utt1atNzY2VrXe6tWrq9YbdMxeA5Ii6EACBB1IgKADCRB0IAGCDiRA0IEECDqQAEEHEpgy6LaX2N52xG2/7fUVegPQkSmvGRcRb0m6QJJsD0naLWlT2bYAdGm6m+6XSnonIt4t0QyAMqYb9DWSHinRCIByWge9uab7NZJG/s/Xmb0G9Km2Axwk6UpJWyPiw8m+GBEbJW2UBv80VWCumc6m+3Visx2Yk1oF3faJki6T9GTZdgCU0HYk078lfbtwLwAK4cg4IAGCDiRA0IEECDqQAEEHEiDoQAIEHUiAoAMJEHQggVKz1z6SNJNz1k+T9HHH7fRDLepRr1a9syPi9KM/WSToM2V7NCJWDFot6lFvtuux6Q4kQNCBBPot6BsHtBb1qDer9fpqHx1AGf22RgdQAEEHEiDoQAIEHUiAoAMJ/AchD47vy2xCkAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gray()\n",
    "# Visualize the first image\n",
    "plt.matshow(digits.images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1325e798",
   "metadata": {},
   "source": [
    "## 3. Obtain the training set and the test set, and execute scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b71e3f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the data and the target values\n",
    "X = np.array(digits.data)\n",
    "Y = np.array(digits.target)\n",
    "\n",
    "# 2. Scale the dataset features of X\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# 3. Split the data into the training set and the test set (ratio being 80:20)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 1/5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bba97b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Obtain the transpose\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c59ad9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Execute the one-hot encoding for both targets\n",
    "one_hot_train = np.zeros((len(Y_train),10))\n",
    "one_hot_train[np.arange(len(Y_train)),Y_train] = 1\n",
    "Y_train = one_hot_train.T\n",
    "\n",
    "one_hot_test = np.zeros((len(Y_test),10))\n",
    "one_hot_test[np.arange(len(Y_test)),Y_test] = 1\n",
    "Y_test = one_hot_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c511227e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1437)\n",
      "(64, 360)\n",
      "(10, 1437)\n",
      "(10, 360)\n"
     ]
    }
   ],
   "source": [
    "# print the shapes to check if the conversions were done properly. \n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fa14a4",
   "metadata": {},
   "source": [
    "## 4. Define necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07cb1a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. function to initialize weights and biases\n",
    "# a,b,c will be the numbers of the neurons in different layers. In this case, a = 64, b = 20, c = 10\n",
    "def initial_params(a,b,c):\n",
    "    W1 = np.random.randn(b, a)\n",
    "    b1 = np.random.randn(b, 1)\n",
    "    W2 = np.random.randn(c, b)\n",
    "    b2 = np.random.randn(c, 1)\n",
    "    \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# 2-1. Sigmoid function\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "# 2-2. Derivative of the sigmoid function\n",
    "def d_sigmoid(Z):\n",
    "    return sigmoid(Z)*(1-sigmoid(Z))\n",
    "\n",
    "# 3-1. ReLU function\n",
    "def ReLU(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "# 3-2. Derivative of the the ReLU function\n",
    "def d_ReLU(Z):\n",
    "    dZ = np.zeros_like(Z)\n",
    "    dZ[Z>0] = 1\n",
    "    return dZ\n",
    "\n",
    "# 4-1. tanh function\n",
    "def tanh(Z):\n",
    "    return np.tanh(Z)\n",
    "\n",
    "# 4-2. Derivative of the tanh function\n",
    "def d_tanh(Z):\n",
    "    return 1-(np.square(tanh(Z)))\n",
    "\n",
    "# 5-1. feed forward function for sigmoid function\n",
    "def forward_sigmoid(W1, b1, W2, b2, X):  \n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "# 5-2. feed forward function for ReLU function\n",
    "def forward_ReLU(W1, b1, W2, b2, X):     \n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = ReLU(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "# 5-3. feed forward function for tanh function\n",
    "def forward_tanh(W1, b1, W2, b2, X): \n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = tanh(Z1)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = tanh(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "# 6-1 backpropagation for sigmoid function\n",
    "def backprop_sigmoid(X, Y, W1, W2, A2, Z2, A1, Z1):\n",
    "    \n",
    "    # dC/dZ2 = dC/dA2 * dA2/dZ2 (10*m)\n",
    "    dZ2 = (A2 - Y) * d_sigmoid(Z2)\n",
    "    # dC/dW2 = dC/dZ2 * dZ2/dW2 (10*30)\n",
    "    dW2 = dZ2.dot(A1.T)\n",
    "    # dC/db2 = dC/dZ2 * dZ2/db2 = dC/dZ2 * 1 (10*1)\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True)\n",
    "    # dC/dZ1 = dC/dZ2 * dZ2/dA1 * dA1/dZ1 = \"dZ2\" * W2 * d_sigmoid (30*m)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * d_sigmoid(Z1)\n",
    "    # dC/dW1 = dC/dZ1 * dZ1/dW1 = \"dZ1\" * X.T (30*64)\n",
    "    dW1 = np.dot(dZ1, X.T)\n",
    "    # dC/db1 = dC/dZ1 * dZ1/db1 = \"dZ1\" * 1 (30*1)\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "# 6-2 backpropagation for ReLU function\n",
    "def backprop_ReLU(X, Y, W1, W2, A2, Z2, A1, Z1):\n",
    "    \n",
    "    # dC/dZ2 = dC/dA2 * dA2/dZ2 (10*m)\n",
    "    dZ2 = (A2 - Y) * d_ReLU(Z2)\n",
    "    # dC/dW2 = dC/dZ2 * dZ2/dW2 (10*30)\n",
    "    dW2 = dZ2.dot(A1.T)\n",
    "    # dC/db2 = dC/dZ2 * dZ2/db2 = dC/dZ2 * 1 (10*1)\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True)\n",
    "    # dC/dZ1 = dC/dZ2 * dZ2/dA1 * dA1/dZ1 = \"dZ2\" * W2 * d_sigmoid (30*m)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * d_ReLU(Z1)\n",
    "    # dC/dW1 = dC/dZ1 * dZ1/dW1 = \"dZ1\" * X.T (30*64)\n",
    "    dW1 = np.dot(dZ1, X.T)\n",
    "    # dC/db1 = dC/dZ1 * dZ1/db1 = \"dZ1\" * 1 (30*1)\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "# 6-2 backpropagation for tanh(x) function\n",
    "def backprop_tanh(X, Y, W1, W2, A2, Z2, A1, Z1):\n",
    "    \n",
    "    # dC/dZ2 = dC/dA2 * dA2/dZ2 (10*m)\n",
    "    dZ2 = (A2 - Y) * d_tanh(Z2)\n",
    "    # dC/dW2 = dC/dZ2 * dZ2/dW2 (10*30)\n",
    "    dW2 = dZ2.dot(A1.T)\n",
    "    # dC/db2 = dC/dZ2 * dZ2/db2 = dC/dZ2 * 1 (10*1)\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True)\n",
    "    # dC/dZ1 = dC/dZ2 * dZ2/dA1 * dA1/dZ1 = \"dZ2\" * W2 * d_sigmoid (30*m)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * d_tanh(Z1)\n",
    "    # dC/dW1 = dC/dZ1 * dZ1/dW1 = \"dZ1\" * X.T (30*64)\n",
    "    dW1 = np.dot(dZ1, X.T)\n",
    "    # dC/db1 = dC/dZ1 * dZ1/db1 = \"dZ1\" * 1 (30*1)\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "# 7. function to update the params\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1    \n",
    "    W2 = W2 - learning_rate * dW2  \n",
    "    b2 = b2 - learning_rate * db2    \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# 8. function to compute the accuracy\n",
    "def accuracy(A2,Y):\n",
    "    return np.sum(np.argmax(A2, axis=0) == np.argmax(Y, axis=0))/len(Y[0])\n",
    "\n",
    "# 9-1. Model training with sigmoid function\n",
    "def NN_training_sigmoid(X, Y, learning_rate, iterations):\n",
    "    \n",
    "    # initialize the weights and biases for 2 layers\n",
    "    W1, b1, W2, b2 = initial_params(64,30,10)\n",
    "    \n",
    "    # Repeat the gradient descent for the number of iterations:\n",
    "    for i in range(1,iterations+1):\n",
    "        # forward feeding\n",
    "        Z1, A1, Z2, A2 = forward_sigmoid(W1, b1, W2, b2, X)\n",
    "        # backpropagation\n",
    "        dW1, db1, dW2, db2 = backprop_sigmoid(X, Y, W1, W2, A2, Z2, A1, Z1)\n",
    "        # update parameters\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "        if (i%100 == 0):\n",
    "            print(\"iteration\", i,\": accuracy:\", accuracy(A2,Y))    \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# 9-2. Model training with ReLU function\n",
    "def NN_training_ReLU(X, Y, learning_rate, iterations):\n",
    "    \n",
    "    # initialize the weights and biases for 2 layers\n",
    "    W1, b1, W2, b2 = initial_params(64,30,10)\n",
    "    \n",
    "    # Repeat the gradient descent for the number of iterations:\n",
    "    for i in range(1,iterations+1):\n",
    "        # forward feeding\n",
    "        Z1, A1, Z2, A2 = forward_ReLU(W1, b1, W2, b2, X)\n",
    "        # backpropagation\n",
    "        dW1, db1, dW2, db2 = backprop_ReLU(X, Y, W1, W2, A2, Z2, A1, Z1)\n",
    "        # update parameters\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "        if (i%100 == 0):\n",
    "            print(\"iteration\", i,\": accuracy:\", accuracy(A2,Y))     \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# 9-3. Model training with tanh function\n",
    "def NN_training_tanh(X, Y, learning_rate, iterations):\n",
    "    \n",
    "    # initialize the weights and biases for 2 layers\n",
    "    W1, b1, W2, b2 = initial_params(64,30,10)\n",
    "    \n",
    "    # Repeat the gradient descent for the number of iterations:\n",
    "    for i in range(1,iterations+1):\n",
    "        # forward feeding\n",
    "        Z1, A1, Z2, A2 = forward_tanh(W1, b1, W2, b2, X)\n",
    "        # backpropagation\n",
    "        dW1, db1, dW2, db2 = backprop_tanh(X, Y, W1, W2, A2, Z2, A1, Z1)\n",
    "        # update parameters\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "        if (i%100 == 0):\n",
    "            print(\"iteration\", i,\": accuracy:\", accuracy(A2,Y))    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8474cfe",
   "metadata": {},
   "source": [
    "## 5. Train and test the neural networks with different activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3455bde",
   "metadata": {},
   "source": [
    "### 1. Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "662b1528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 : accuracy: 0.34725121781489215\n",
      "iteration 200 : accuracy: 0.430062630480167\n",
      "iteration 300 : accuracy: 0.5031315240083507\n",
      "iteration 400 : accuracy: 0.5741127348643006\n",
      "iteration 500 : accuracy: 0.6082115518441197\n",
      "iteration 600 : accuracy: 0.6249130132219902\n",
      "iteration 700 : accuracy: 0.6443980514961726\n",
      "iteration 800 : accuracy: 0.6506610995128741\n",
      "iteration 900 : accuracy: 0.6527487821851079\n",
      "iteration 1000 : accuracy: 0.6569241475295755\n"
     ]
    }
   ],
   "source": [
    "# 1. Train the model with the training set (learning rate = 0.001, iterations = 1000)\n",
    "W1, b1, W2, b2 = NN_training_sigmoid(X_train,Y_train, 0.001, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06d36f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.6361111111111111\n"
     ]
    }
   ],
   "source": [
    "# 2. Test the model with the test set\n",
    "Z1, A1, Z2, A2 = forward_sigmoid(W1, b1, W2, b2, X_test)\n",
    "print(\"accuracy: \", accuracy(A2,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8927cf1",
   "metadata": {},
   "source": [
    "As shown above, the neural network with sigmoid function as activation function shows a high accuracy for both training set and test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876a7eb9",
   "metadata": {},
   "source": [
    "### 2. ReLU function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b2ef889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 : accuracy: 0.03479471120389701\n",
      "iteration 200 : accuracy: 0.04175365344467641\n",
      "iteration 300 : accuracy: 0.04384133611691023\n",
      "iteration 400 : accuracy: 0.04384133611691023\n",
      "iteration 500 : accuracy: 0.04523312456506611\n",
      "iteration 600 : accuracy: 0.04314544189283229\n",
      "iteration 700 : accuracy: 0.03897007654836465\n",
      "iteration 800 : accuracy: 0.03688239387613083\n",
      "iteration 900 : accuracy: 0.03479471120389701\n",
      "iteration 1000 : accuracy: 0.03479471120389701\n"
     ]
    }
   ],
   "source": [
    "# 1. Train the model with the training set (learning rate = 0.00001, iterations = 1000)\n",
    "# Note that the learning rate was set very low, as the error will be very high. Further explanations shown below.\n",
    "W1, b1, W2, b2 = NN_training_ReLU(X_train,Y_train, 0.00000001, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15995b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.025\n"
     ]
    }
   ],
   "source": [
    "# 2. Test the model with the test set\n",
    "Z1, A1, Z2, A2 = forward_ReLU(W1, b1, W2, b2, X_test)\n",
    "print(\"accuracy: \", accuracy(A2,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae277f80",
   "metadata": {},
   "source": [
    "As shown above, the neural network with ReLU function as activation function shows a low accuracy for both training set and test sets.\n",
    "\n",
    "More importantly, the accuracy remains the same during the learning, which indicates that the model barely learns or updates the weights or biases but remains the initial value.\n",
    "\n",
    "This is because this neural network **is using the ReLU function as the activation function of the output layer.** \n",
    "\n",
    "The range of ReLU function is [0,+inf], while the target y value is either 0 to 1. \n",
    "\n",
    "Note that we are using the MSE to compute the error but obtaining the maximum activation number as the final prediction. This is self-contradictory, which leads to the situation that the program never effectively updates the params in the learning process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517b056c",
   "metadata": {},
   "source": [
    "### 3. tanh function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cda470b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 : accuracy: 0.38552540013917885\n",
      "iteration 200 : accuracy: 0.5539318023660403\n",
      "iteration 300 : accuracy: 0.6938065414057063\n",
      "iteration 400 : accuracy: 0.7689631176061239\n",
      "iteration 500 : accuracy: 0.8121085594989561\n",
      "iteration 600 : accuracy: 0.8434237995824635\n",
      "iteration 700 : accuracy: 0.8636047320807237\n",
      "iteration 800 : accuracy: 0.8768267223382046\n",
      "iteration 900 : accuracy: 0.8956158663883089\n",
      "iteration 1000 : accuracy: 0.9088378566457899\n"
     ]
    }
   ],
   "source": [
    "# 1. Train the model with the training set (learning rate = 0.001, iterations = 1000)\n",
    "W1, b1, W2, b2 = NN_training_tanh(X_train,Y_train, 0.001, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47fabe46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8805555555555555\n"
     ]
    }
   ],
   "source": [
    "Z1, A1, Z2, A2 = forward_tanh(W1, b1, W2, b2, X_test)\n",
    "print(\"accuracy: \", accuracy(A2,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a84a2c",
   "metadata": {},
   "source": [
    "As shown above, similar to the one with sigmoid function, the neural network using the tanh function as the showed hige accuracy for both training set and the test set as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd4f470",
   "metadata": {},
   "source": [
    "### 4. Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97112188",
   "metadata": {},
   "source": [
    "Before get started, ignore the NN using the ReLU function as it shows poor performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58ccb86",
   "metadata": {},
   "source": [
    "**First**, As I found the accuracy varies significantly when the iteration number is 1000. So, I decide to increase it to 5000 for both sigmoid NN and tanh network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f44885d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 : accuracy: 0.615866388308977\n",
      "iteration 200 : accuracy: 0.7341684064022269\n",
      "iteration 300 : accuracy: 0.7919276270006959\n",
      "iteration 400 : accuracy: 0.8761308281141267\n",
      "iteration 500 : accuracy: 0.9123173277661796\n",
      "iteration 600 : accuracy: 0.9283228949199722\n",
      "iteration 700 : accuracy: 0.9401530967292971\n",
      "iteration 800 : accuracy: 0.9457202505219207\n",
      "iteration 900 : accuracy: 0.9491997216423104\n",
      "iteration 1000 : accuracy: 0.953375086986778\n",
      "iteration 1100 : accuracy: 0.9596381350034795\n",
      "iteration 1200 : accuracy: 0.9659011830201809\n",
      "iteration 1300 : accuracy: 0.9665970772442589\n",
      "iteration 1400 : accuracy: 0.9700765483646486\n",
      "iteration 1500 : accuracy: 0.9721642310368824\n",
      "iteration 1600 : accuracy: 0.9742519137091162\n",
      "iteration 1700 : accuracy: 0.9756437021572721\n",
      "iteration 1800 : accuracy: 0.9756437021572721\n",
      "iteration 1900 : accuracy: 0.97633959638135\n",
      "iteration 2000 : accuracy: 0.9798190675017397\n",
      "iteration 2100 : accuracy: 0.9812108559498957\n",
      "iteration 2200 : accuracy: 0.9812108559498957\n",
      "iteration 2300 : accuracy: 0.9819067501739736\n",
      "iteration 2400 : accuracy: 0.9819067501739736\n",
      "iteration 2500 : accuracy: 0.9826026443980515\n",
      "iteration 2600 : accuracy: 0.9832985386221295\n",
      "iteration 2700 : accuracy: 0.9832985386221295\n",
      "iteration 2800 : accuracy: 0.9839944328462074\n",
      "iteration 2900 : accuracy: 0.9846903270702854\n",
      "iteration 3000 : accuracy: 0.9853862212943633\n",
      "iteration 3100 : accuracy: 0.9853862212943633\n",
      "iteration 3200 : accuracy: 0.9853862212943633\n",
      "iteration 3300 : accuracy: 0.9860821155184412\n",
      "iteration 3400 : accuracy: 0.9860821155184412\n",
      "iteration 3500 : accuracy: 0.9867780097425192\n",
      "iteration 3600 : accuracy: 0.9867780097425192\n",
      "iteration 3700 : accuracy: 0.9874739039665971\n",
      "iteration 3800 : accuracy: 0.988169798190675\n",
      "iteration 3900 : accuracy: 0.988169798190675\n",
      "iteration 4000 : accuracy: 0.988865692414753\n",
      "iteration 4100 : accuracy: 0.9895615866388309\n",
      "iteration 4200 : accuracy: 0.9895615866388309\n",
      "iteration 4300 : accuracy: 0.9895615866388309\n",
      "iteration 4400 : accuracy: 0.9902574808629089\n",
      "iteration 4500 : accuracy: 0.9902574808629089\n",
      "iteration 4600 : accuracy: 0.9909533750869868\n",
      "iteration 4700 : accuracy: 0.9909533750869868\n",
      "iteration 4800 : accuracy: 0.9909533750869868\n",
      "iteration 4900 : accuracy: 0.9909533750869868\n",
      "iteration 5000 : accuracy: 0.9909533750869868\n",
      "accuracy:  0.9527777777777777\n"
     ]
    }
   ],
   "source": [
    "# 1. Train the model with the training set (learning rate = 0.001, iterations = 5000)\n",
    "W1, b1, W2, b2 = NN_training_sigmoid(X_train,Y_train, 0.001, 5000)\n",
    "# 2. Test the model with the test set\n",
    "Z1, A1, Z2, A2 = forward_sigmoid(W1, b1, W2, b2, X_test)\n",
    "print(\"accuracy: \", accuracy(A2,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbf717f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 : accuracy: 0.3117606123869172\n",
      "iteration 200 : accuracy: 0.510786360473208\n",
      "iteration 300 : accuracy: 0.662491301322199\n",
      "iteration 400 : accuracy: 0.7807933194154488\n",
      "iteration 500 : accuracy: 0.8427279053583855\n",
      "iteration 600 : accuracy: 0.8434237995824635\n",
      "iteration 700 : accuracy: 0.872651356993737\n",
      "iteration 800 : accuracy: 0.8907446068197634\n",
      "iteration 900 : accuracy: 0.8970076548364648\n",
      "iteration 1000 : accuracy: 0.9109255393180237\n",
      "iteration 1100 : accuracy: 0.9137091162143354\n",
      "iteration 1200 : accuracy: 0.9171885873347251\n",
      "iteration 1300 : accuracy: 0.9220598469032707\n",
      "iteration 1400 : accuracy: 0.9234516353514266\n",
      "iteration 1500 : accuracy: 0.930410577592206\n",
      "iteration 1600 : accuracy: 0.9359777313848295\n",
      "iteration 1700 : accuracy: 0.942936673625609\n",
      "iteration 1800 : accuracy: 0.942936673625609\n",
      "iteration 1900 : accuracy: 0.9485038274182325\n",
      "iteration 2000 : accuracy: 0.9485038274182325\n",
      "iteration 2100 : accuracy: 0.9498956158663883\n",
      "iteration 2200 : accuracy: 0.9505915100904663\n",
      "iteration 2300 : accuracy: 0.953375086986778\n",
      "iteration 2400 : accuracy: 0.9547668754349339\n",
      "iteration 2500 : accuracy: 0.9561586638830898\n",
      "iteration 2600 : accuracy: 0.9596381350034795\n",
      "iteration 2700 : accuracy: 0.9589422407794015\n",
      "iteration 2800 : accuracy: 0.9610299234516354\n",
      "iteration 2900 : accuracy: 0.9596381350034795\n",
      "iteration 3000 : accuracy: 0.9617258176757133\n",
      "iteration 3100 : accuracy: 0.9624217118997912\n",
      "iteration 3200 : accuracy: 0.9631176061238692\n",
      "iteration 3300 : accuracy: 0.9624217118997912\n",
      "iteration 3400 : accuracy: 0.9603340292275574\n",
      "iteration 3500 : accuracy: 0.9617258176757133\n",
      "iteration 3600 : accuracy: 0.9638135003479471\n",
      "iteration 3700 : accuracy: 0.9638135003479471\n",
      "iteration 3800 : accuracy: 0.9617258176757133\n",
      "iteration 3900 : accuracy: 0.9631176061238692\n",
      "iteration 4000 : accuracy: 0.964509394572025\n",
      "iteration 4100 : accuracy: 0.9631176061238692\n",
      "iteration 4200 : accuracy: 0.9638135003479471\n",
      "iteration 4300 : accuracy: 0.964509394572025\n",
      "iteration 4400 : accuracy: 0.9665970772442589\n",
      "iteration 4500 : accuracy: 0.965205288796103\n",
      "iteration 4600 : accuracy: 0.9665970772442589\n",
      "iteration 4700 : accuracy: 0.9659011830201809\n",
      "iteration 4800 : accuracy: 0.9672929714683368\n",
      "iteration 4900 : accuracy: 0.9686847599164927\n",
      "iteration 5000 : accuracy: 0.9707724425887265\n",
      "accuracy:  0.9194444444444444\n"
     ]
    }
   ],
   "source": [
    "# 1. Train the model with the training set (learning rate = 0.001, iterations = 5000)\n",
    "W1, b1, W2, b2 = NN_training_tanh(X_train,Y_train, 0.001, 5000)\n",
    "# 2. Test the model with the test set\n",
    "Z1, A1, Z2, A2 = forward_tanh(W1, b1, W2, b2, X_test)\n",
    "print(\"accuracy: \", accuracy(A2,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ffab0e",
   "metadata": {},
   "source": [
    "Although the accuracy results may vary for different trials, we can see that, in general, 5000 times of iteration is sufficient to stably obtain the high performance for both sigmoid NN and tanh NN. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b73693",
   "metadata": {},
   "source": [
    "**Second**, I supposed the learning rate of 0.001 was so low that the number of iterations was not enough to stably produce effective NNs. Therefore, I raised it up to 0.01, while the iterations remaining 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b45cc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 : accuracy: 0.9380654140570633\n",
      "iteration 200 : accuracy: 0.9714683368128044\n",
      "iteration 300 : accuracy: 0.9819067501739736\n",
      "iteration 400 : accuracy: 0.9895615866388309\n",
      "iteration 500 : accuracy: 0.9895615866388309\n",
      "iteration 600 : accuracy: 0.9916492693110647\n",
      "iteration 700 : accuracy: 0.9923451635351427\n",
      "iteration 800 : accuracy: 0.9937369519832986\n",
      "iteration 900 : accuracy: 0.9937369519832986\n",
      "iteration 1000 : accuracy: 0.9944328462073765\n",
      "accuracy:  0.9527777777777777\n"
     ]
    }
   ],
   "source": [
    "# 1. Train the model with the training set (learning rate = 0.1, iterations = 1000)\n",
    "W1, b1, W2, b2 = NN_training_sigmoid(X_train,Y_train, 0.01, 1000)\n",
    "# 2. Test the model with the test set\n",
    "Z1, A1, Z2, A2 = forward_sigmoid(W1, b1, W2, b2, X_test)\n",
    "print(\"accuracy: \", accuracy(A2,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c06fb4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 : accuracy: 0.6805845511482255\n",
      "iteration 200 : accuracy: 0.8761308281141267\n",
      "iteration 300 : accuracy: 0.9839944328462074\n",
      "iteration 400 : accuracy: 0.9874739039665971\n",
      "iteration 500 : accuracy: 0.988169798190675\n",
      "iteration 600 : accuracy: 0.988865692414753\n",
      "iteration 700 : accuracy: 0.9895615866388309\n",
      "iteration 800 : accuracy: 0.9895615866388309\n",
      "iteration 900 : accuracy: 0.9895615866388309\n",
      "iteration 1000 : accuracy: 0.9902574808629089\n",
      "accuracy:  0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "# 1. Train the model with the training set (learning rate = 0.1, iterations = 1000)\n",
    "W1, b1, W2, b2 = NN_training_sigmoid(X_train,Y_train, 0.01, 1000)\n",
    "# 2. Test the model with the test set\n",
    "Z1, A1, Z2, A2 = forward_sigmoid(W1, b1, W2, b2, X_test)\n",
    "print(\"accuracy: \", accuracy(A2,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dd28ae",
   "metadata": {},
   "source": [
    "As shown above, the accuracy improves much faster when the learning rate is increased to 0.01. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821eee24",
   "metadata": {},
   "source": [
    "**If I should choose only one between increasing the iterations to 5000 and increasing the rate to 0.01, I would choose the latter since this is more computationally efficient.** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
